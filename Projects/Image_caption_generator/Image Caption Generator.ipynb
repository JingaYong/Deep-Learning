{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6c048f3e-8744-43db-87d5-f8f8cd8860a8",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da74e02-f7fd-4ae9-9903-d1c709f2e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split as tt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "# from keras.utils import load_img,img_to_array,plot_model\n",
    "from keras.layers import LSTM, Dense, Input, Embedding,Dropout, add\n",
    "from keras.models import Sequential, Model\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from PIL import Image\n",
    "import warnings\n",
    "# import cv2 as cv\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81f74374-7bd8-4212-aa41-b4a34ae42765",
   "metadata": {},
   "source": [
    "Initialize vars and set train and test dir paths\n",
    "\n",
    "Load base model - VGG16(remove last layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670d6b39-c193-4eef-99fa-c68c82eec93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_dir = 'Images and Captions'\n",
    "# load vgg16 model\n",
    "model = VGG16()\n",
    "# restructure the model\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "# summarize\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e3b17b6-5c8a-4923-8823-ad0f8f28150a",
   "metadata": {},
   "source": [
    "Extract features from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49600650-6dcf-4899-b9d5-e72b5513cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "directory = os.path.join(base_dir, 'Images')\n",
    "count = 0\n",
    "for img_name in tqdm(os.listdir(directory)):\n",
    "    count += 1\n",
    "    # load the image from file\n",
    "    img_path = directory + '/' + img_name\n",
    "    image = keras.utils.load_img(img_path, target_size=(224, 224))\n",
    "    # convert image pixels to numpy array\n",
    "    image = keras.utils.img_to_array(image)\n",
    "    # reshape data for model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # preprocess image for vgg\n",
    "    image = preprocess_input(image)\n",
    "    # extract features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    # get image ID\n",
    "    image_id = img_name.split('.')[0]\n",
    "    # store feature\n",
    "    features[image_id] = feature\n",
    "\n",
    "# store features in pickle\n",
    "pickle.dump(features, open(os.path.join(work_dir, 'features.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e935ee04-0dd4-47ac-b92e-43c29bd2bcdc",
   "metadata": {},
   "source": [
    "Load features from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38fbff68-6d6e-4278-a282-d354e93721d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features.pkl', 'rb') as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b193ca1e-85f7-408f-8d61-145cc92a59d2",
   "metadata": {},
   "source": [
    "Load the Captions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dda12c9-0be3-4031-b9c7-e071ecc3cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(base_dir, 'captions.txt'), 'r') as f:\n",
    "    next(f)\n",
    "    captions_doc = f.read()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ccbd66fe-9d93-4d97-a6b4-ec3b544de97c",
   "metadata": {},
   "source": [
    "Create mapping of image to captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63769e42-08f0-4dc8-aaf0-1a885cd19a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "# process lines\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    # split the line by comma(,)\n",
    "    tokens = line.split(',')\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    image_id, caption = tokens[0], tokens[1:]\n",
    "    # remove extension from image ID\n",
    "    image_id = image_id.split('.')[0]\n",
    "    # convert caption list to string - because the sentences with commas will be split to separate strings in a list\n",
    "    caption = \" \".join(caption)\n",
    "    # create list if needed\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    # store the caption\n",
    "    mapping[image_id].append(caption)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b01c6dec-93b0-4906-9ffb-18cf33dcbbc6",
   "metadata": {},
   "source": [
    "Preprocess Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd712a7d-8853-406d-8fe8-5fad2b44172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(mappin):\n",
    "    for key, captions in mappin.items():\n",
    "        for i in range(len(captions)):\n",
    "            # take one caption at a time\n",
    "            caption = captions[i]\n",
    "            # preprocessing steps\n",
    "            # convert to lowercase\n",
    "            caption = caption.lower()\n",
    "            # delete digits, special chars, etc.,\n",
    "            caption = re.sub('[^A-Za-z]',' ',caption)\n",
    "            # # delete additional spaces\n",
    "            caption = caption.replace('\\s+',' ')\n",
    "            # print(caption)\n",
    "            # add start and end tags to the caption\n",
    "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) +' endseq'\n",
    "            captions[i] = caption\n",
    "\n",
    "clean(mapping)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3dac41b-866c-4b29-8287-e2b3186aed79",
   "metadata": {},
   "source": [
    "Append all captions for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4da11234-691e-4996-991a-9b85e8bcc5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        all_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925d175-4a8b-43f4-8ffa-a9f45aa9a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "412233cd-23d8-473f-a4c1-d0533be5e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# print(vocab_size) # 8427\n",
    "# print(tokenizer.word_index)\n",
    "\n",
    "# get maximum length of the caption available\n",
    "max_length = max(len(caption.split()) for caption in all_captions)\n",
    "# print(max_length) # 35\n",
    "\n",
    "image_ids = list(mapping.keys())\n",
    "split = int(len(image_ids) * 0.90)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e477483-4f44-4360-8c85-0bdb41a32195",
   "metadata": {},
   "source": [
    "Data Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39c9f8cc-8ab5-4c40-b041-19f349f6d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    # loop over images\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n += 1\n",
    "            captions = mapping[key]\n",
    "            # process each caption\n",
    "            for caption in captions:\n",
    "                # encode the sequence - [0] becz of 2d list\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                # split the sequence into X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pairs\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence - default is 'pre'=> list of 0's before the remaining in_seq list\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence-#rows of o/p = #elements in out_seq n #cols = #classes\n",
    "                    out_seq = keras.utils.to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                    # store the sequences\n",
    "                    X1.append(features[key][0])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            if n == batch_size:\n",
    "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "                yield [X1, X2], y\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n = 0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e9d1202-2c88-44b1-91e9-dcf0da85463b",
   "metadata": {},
   "source": [
    "Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6d5c0-1548-4523-999c-6b684bdbb9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation - encoder model\n",
    "# image feature layers\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.4)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "# sequence feature layers\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.4)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# plot the model\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99151fbf-fecd-4f08-99c1-0f648c5cd2c1",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a5192-d035-4037-acf1-57ed57efdf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "steps = len(train) // batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    # create data generator\n",
    "    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
    "    # fit for one epoch\n",
    "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05a2b240-0110-4850-a8ca-1bea98071d2c",
   "metadata": {},
   "source": [
    "Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf043a70-0fef-484f-9d18-52eeab6a239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(work_dir + '/img_capt.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74c981de-e019-4f45-84c4-31d7530c8829",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17352393-b08f-49a2-b788-8a4ef2018b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('img_capt.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5abdaff7-4f3d-44bd-a999-0e63f47575d1",
   "metadata": {},
   "source": [
    "Generate  Captions for the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75cf3d41-8fc1-488a-b7d4-889a40f196f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c0b6492-369e-4589-952b-b3021a9dea9d",
   "metadata": {},
   "source": [
    "Generate caption for an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66ac13ec-77a7-437a-aea6-7ffa15928e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    # add start tag for generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the max length of sequence\n",
    "    for i in range(max_length):\n",
    "        # encode input sequence - list of numbers from the vocab\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad the sequence\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([image, sequence], verbose=0)\n",
    "        # get index with high probability - list the len of vocab size\n",
    "        yhat = np.argmax(yhat)\n",
    "        # convert index to word\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        # stop if word not found\n",
    "        if word is None:\n",
    "            break\n",
    "        # append word as input for generating next word\n",
    "        in_text += \" \" + word\n",
    "        # stop if we reach end tag\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19ecf2a6-c93a-428d-a562-1caeddcb624c",
   "metadata": {},
   "source": [
    "Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc6d570-caf4-49ed-a3d6-551908708a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check bleu score - just a metric\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "# validate with test data\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "for key in tqdm(test):\n",
    "    # get actual caption\n",
    "    captions = mapping[key]\n",
    "    # predict the caption for image\n",
    "    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n",
    "    # split into words\n",
    "    actual_captions = [caption.split() for caption in captions]\n",
    "    y_pred = y_pred.split()\n",
    "    # append to the list\n",
    "    actual.append(actual_captions)\n",
    "    predicstd.append(y_pred)\n",
    "\n",
    "# calcuate BLEU score\n",
    "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1e0d614-a742-4b9f-895e-b94177ecae8f",
   "metadata": {},
   "source": [
    "Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175679a9-0c6a-43a1-b494-f49ce16b1845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_name):\n",
    "    # load the image\n",
    "    # image_name = \"1001773457_577c3a7d70.jpg\"\n",
    "    image_id = image_name.split('.')[0]\n",
    "    img_path = os.path.join(base_dir, \"Images\", image_name)\n",
    "    # image = Image.open(img_path)\n",
    "    img = cv.imread(img_path)\n",
    "    captions = mapping[image_id]\n",
    "    print('Actual')\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "    # predict the caption\n",
    "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
    "    print('Predicted')\n",
    "    print(y_pred)\n",
    "    # plt.imshow(image)\n",
    "    cv.imshow('Img',img)\n",
    "    cv.waitKey(0)\n",
    "\n",
    "\n",
    "generate_caption(\"173467821_73647.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
